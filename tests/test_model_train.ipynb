{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\samuelborder\\Desktop\\HIVE_Stuff\\FUSION\\Tools\\fusion-tools\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Importing packages\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "sys.path.append('../src/')\n",
    "#!{sys.executable} -m pip install segmentation-models-pytorch plotly kaleido==0.1.0 albumentations\n",
    "#!{sys.executable} -m pip install nbformat\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import segmentation_models_pytorch as smp\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor, Normalize\n",
    "from torchvision.transforms.v2 import RandomHorizontalFlip, RandomVerticalFlip, ElasticTransform, ColorJitter, Compose\n",
    "import albumentations as A\n",
    "\n",
    "from typing import Callable, List\n",
    "from fusion_tools.dataset import ClassificationDataset\n",
    "from fusion_tools.utils.shapes import load_histomics\n",
    "from math import floor\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CellClassificationModel(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 output_size: int = 2,\n",
    "                 simple: bool = True):\n",
    "        super().__init__()\n",
    "        self.output_size = output_size\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.simple = simple\n",
    "        self.fc1 = torch.nn.LazyLinear(120)\n",
    "        self.fc2 = torch.nn.Linear(120,84)\n",
    "        self.fc3 = torch.nn.Linear(84,self.output_size)\n",
    "        \n",
    "        if self.simple:\n",
    "            self.conv1 = torch.nn.Conv2d(3,6,5)\n",
    "            self.pool = torch.nn.MaxPool2d(2,2)\n",
    "            self.conv2 = torch.nn.Conv2d(6,16,5)\n",
    "\n",
    "            self.conv_layers = torch.nn.Sequential(\n",
    "                self.conv1,\n",
    "                torch.nn.ReLU(inplace=True),\n",
    "                self.pool,\n",
    "                self.conv2,\n",
    "                torch.nn.ReLU(inplace=True),\n",
    "                self.pool\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            model_weights = torchvision.models.EfficientNet_V2_S_Weights\n",
    "            self.conv_layers = torchvision.models.efficientnet_v2_s(weights = model_weights.DEFAULT)\n",
    "\n",
    "        self.linear_layers = torch.nn.Sequential(\n",
    "            self.fc1,\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            self.fc2,\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            self.fc3\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        \n",
    "        output = self.conv_layers(input)\n",
    "        output = torch.flatten(output,1)\n",
    "        output = torch.nn.Sigmoid()(self.linear_layers(output))        \n",
    "        return output\n",
    "\n",
    "\n",
    "def cell_percentages(inp:list):\n",
    "    if len(inp)>0:\n",
    "        return torch.from_numpy(np.array([sum([-1*(i-1) for i in inp])/len(inp), sum(inp)/len(inp)]))\n",
    "    else:\n",
    "        return torch.from_numpy(np.array([1.0, 0.0]))\n",
    "\n",
    "def train(train_data, val_data, model, optimizer, loss, output_dir):\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    loss.to(device)\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    epoch_num = 5\n",
    "    train_loss = 0\n",
    "    val_loss = 0\n",
    "    losses = []\n",
    "    train_step_count = 0\n",
    "    with tqdm(total = epoch_num, position = 0, leave = True, file = sys.stdout) as pbar:\n",
    "        for i in range(0,epoch_num):\n",
    "            for step, (train_imgs,train_labels) in enumerate(train_data):\n",
    "                model.train()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                pbar.set_description(f'Epoch: {i}/{epoch_num}, Train Step: {step}, Train/Val Loss: {round(train_loss,4)}/{round(val_loss,4)}')\n",
    "\n",
    "                train_imgs = train_imgs.to(device)\n",
    "                train_labels = train_labels.to(device)\n",
    "\n",
    "                train_pred = model(train_imgs)\n",
    "\n",
    "                train_loss = loss(train_pred, train_labels)\n",
    "                train_loss.backward()\n",
    "                train_loss = train_loss.item()\n",
    "\n",
    "                optimizer.step()\n",
    "                losses.append({\n",
    "                    'step': train_step_count+step, 'loss': train_loss, 'Train/Val': 'train'\n",
    "                })\n",
    "\n",
    "                # Performing validation\n",
    "                with torch.no_grad():\n",
    "                    model.eval()\n",
    "                    val_imgs, val_labels = next(iter(val_data))\n",
    "\n",
    "                    val_imgs = val_imgs.to(device)\n",
    "                    val_labels = val_labels.to(device)\n",
    "\n",
    "                    val_pred = model(val_imgs)\n",
    "                    val_loss = loss(val_pred, val_labels)\n",
    "                    val_loss = val_loss.item()\n",
    "\n",
    "                    losses.append(\n",
    "                        {'step': train_step_count+step, 'loss': val_loss, 'Train/Val': 'val'}\n",
    "                    )\n",
    "\n",
    "            train_step_count+=step\n",
    "            torch.save(model.state_dict(), output_dir+'Classification_Model.pth')\n",
    "            loss_df = pd.DataFrame.from_records(losses)\n",
    "            loss_df.to_csv(output_dir+'Loss.csv')\n",
    "            vis_loss(loss_df,output_dir)\n",
    "\n",
    "            pbar.update(1)\n",
    "\n",
    "\n",
    "        pbar.update(1)\n",
    "        torch.save(model.state_dict(), output_dir+'Classification_Model.pth')\n",
    "        loss_df = pd.DataFrame.from_records(losses)\n",
    "        loss_df.to_csv(output_dir+'Loss.csv')\n",
    "        vis_loss(loss_df,output_dir)\n",
    "        pbar.close()\n",
    "\n",
    "def vis_loss(loss_df, output_dir):\n",
    "    \n",
    "    plot = px.line(\n",
    "        data_frame=loss_df,\n",
    "        x = 'step',\n",
    "        y = 'loss',\n",
    "        color = 'Train/Val'\n",
    "    )\n",
    "\n",
    "    plot.write_image(output_dir+'Loss_Plot.png')\n",
    "\n",
    "def test(model_path, output_size, holdout_data, n):\n",
    "\n",
    "    model = CellClassificationModel(\n",
    "        output_size = output_size\n",
    "    )\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "        model.load_state_dict(torch.load(model_path,weights_only=True))\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "        model.load_state_dict(torch.load(model_path,weights_only=True,map_location = torch.device('cpu')))\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    test_dataloader = iter(DataLoader(holdout_data,batch_size=1,shuffle=False))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx in range(n):\n",
    "            image,gt = next(test_dataloader)\n",
    "            pred = model(image.to(device)).detach().numpy()\n",
    "\n",
    "            image = np.moveaxis(np.squeeze(image.detach().numpy()),source=0,destination=-1)\n",
    "\n",
    "            plot = go.Figure(\n",
    "                px.imshow(image)\n",
    "            )\n",
    "            plot.show()\n",
    "\n",
    "            print(f'Predicted: {pred}, GT: {gt.numpy()}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting dataset construction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37795/37795 [05:31<00:00, 114.09it/s]\n",
      "100%|██████████| 12599/12599 [01:33<00:00, 134.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets prepared!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "slides = [\n",
    "    \"C:\\\\Users\\\\samuelborder\\\\Desktop\\\\HIVE_Stuff\\\\FUSION\\\\Test Upload\\\\Xenium_Data\\\\40775.tif\"\n",
    "]\n",
    "annotations = load_histomics('C:\\\\Users\\\\samuelborder\\\\Desktop\\\\HIVE_Stuff\\\\FUSION\\\\Test Upload\\\\Xenium_Data\\\\Cells.json')[0]\n",
    "\n",
    "# Splitting the data into training and validation sets:\n",
    "total_anns = len(annotations['features'])\n",
    "train_test_split = 0.75\n",
    "train_annotations = [{\n",
    "    'type': 'FeatureCollection',\n",
    "    'features': annotations['features'][:floor(train_test_split*total_anns)],\n",
    "    'properties': {'name': 'Train Cells'}\n",
    "}]\n",
    "\n",
    "val_annotations = [{\n",
    "    'type': 'FeatureCollection',\n",
    "    'features': annotations['features'][floor(train_test_split*total_anns):],\n",
    "    'properties': {'name': 'Validation Cells'}\n",
    "}]\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "# Image augmentations (Normalization means and std are optimized for ImageNet pre-trained models)\n",
    "train_transforms = Compose([\n",
    "    RandomHorizontalFlip(p=0.5),\n",
    "    RandomVerticalFlip(p=0.5),\n",
    "    ColorJitter(),\n",
    "    ToTensor(),\n",
    "    Normalize(mean = [0.485, 0.456, 0.406], std = [0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "val_transforms = Compose([\n",
    "    ToTensor(),\n",
    "    Normalize(mean = [0.485,0.456, 0.406], std = [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Converts list of labels per cell to proportions of each cell type\n",
    "label_transform = lambda imm_list: cell_percentages(imm_list)\n",
    "\n",
    "print('Starting dataset construction')\n",
    "train_data = ClassificationDataset(\n",
    "    slides = slides,\n",
    "    annotations = train_annotations,\n",
    "    label_property = 'Main_Cell_Types --> IMM',\n",
    "    transforms = train_transforms,\n",
    "    label_transforms = label_transform,\n",
    "    use_cache = False,\n",
    "    use_parallel=False,\n",
    "    patch_mode = 'centered_bbox',\n",
    "    verbose = True\n",
    ")\n",
    "\n",
    "val_data = ClassificationDataset(\n",
    "    slides = slides,\n",
    "    annotations = val_annotations,\n",
    "    label_property = 'Main_Cell_Types --> IMM',\n",
    "    transforms = val_transforms,\n",
    "    label_transforms = label_transform,\n",
    "    use_cache = False,\n",
    "    use_parallel=False,\n",
    "    patch_mode = 'centered_bbox',\n",
    "    verbose = True\n",
    ")\n",
    "\n",
    "print('Datasets prepared!')\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size = batch_size, shuffle = True)\n",
    "val_dataloader = DataLoader(val_data, batch_size = batch_size, shuffle = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Invalid Weight class provided; expected ResNet50_Weights but received EfficientNet_V2_S_Weights.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mCellClassificationModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43msimple\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m      4\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam([\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28mdict\u001b[39m(params \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mparameters(), lr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5e-5\u001b[39m, weight_decay \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.00001\u001b[39m)\n\u001b[0;32m      8\u001b[0m ])\n\u001b[0;32m     10\u001b[0m loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n",
      "Cell \u001b[1;32mIn[2], line 29\u001b[0m, in \u001b[0;36mCellClassificationModel.__init__\u001b[1;34m(self, output_size, simple)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     28\u001b[0m     model_weights \u001b[38;5;241m=\u001b[39m torchvision\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mEfficientNet_V2_S_Weights\n\u001b[1;32m---> 29\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_layers \u001b[38;5;241m=\u001b[39m \u001b[43mtorchvision\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresnet50\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel_weights\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDEFAULT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear_layers \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mSequential(\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1,\n\u001b[0;32m     33\u001b[0m     torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mReLU(inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc3\n\u001b[0;32m     37\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\samuelborder\\Desktop\\HIVE_Stuff\\FUSION\\Tools\\fusion-tools\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:142\u001b[0m, in \u001b[0;36mkwonly_to_pos_or_kw.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    135\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    136\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msequence_to_str(\u001b[38;5;28mtuple\u001b[39m(keyword_only_kwargs\u001b[38;5;241m.\u001b[39mkeys()),\u001b[38;5;250m \u001b[39mseparate_last\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mand \u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m as positional \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    137\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    138\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    139\u001b[0m     )\n\u001b[0;32m    140\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mupdate(keyword_only_kwargs)\n\u001b[1;32m--> 142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\samuelborder\\Desktop\\HIVE_Stuff\\FUSION\\Tools\\fusion-tools\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:228\u001b[0m, in \u001b[0;36mhandle_legacy_interface.<locals>.outer_wrapper.<locals>.inner_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    225\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m kwargs[pretrained_param]\n\u001b[0;32m    226\u001b[0m     kwargs[weights_param] \u001b[38;5;241m=\u001b[39m default_weights_arg\n\u001b[1;32m--> 228\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbuilder\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\samuelborder\\Desktop\\HIVE_Stuff\\FUSION\\Tools\\fusion-tools\\.venv\\Lib\\site-packages\\torchvision\\models\\resnet.py:761\u001b[0m, in \u001b[0;36mresnet50\u001b[1;34m(weights, progress, **kwargs)\u001b[0m\n\u001b[0;32m    734\u001b[0m \u001b[38;5;129m@register_model\u001b[39m()\n\u001b[0;32m    735\u001b[0m \u001b[38;5;129m@handle_legacy_interface\u001b[39m(weights\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpretrained\u001b[39m\u001b[38;5;124m\"\u001b[39m, ResNet50_Weights\u001b[38;5;241m.\u001b[39mIMAGENET1K_V1))\n\u001b[0;32m    736\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mresnet50\u001b[39m(\u001b[38;5;241m*\u001b[39m, weights: Optional[ResNet50_Weights] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, progress: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResNet:\n\u001b[0;32m    737\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"ResNet-50 from `Deep Residual Learning for Image Recognition <https://arxiv.org/abs/1512.03385>`__.\u001b[39;00m\n\u001b[0;32m    738\u001b[0m \n\u001b[0;32m    739\u001b[0m \u001b[38;5;124;03m    .. note::\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    759\u001b[0m \u001b[38;5;124;03m        :members:\u001b[39;00m\n\u001b[0;32m    760\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 761\u001b[0m     weights \u001b[38;5;241m=\u001b[39m \u001b[43mResNet50_Weights\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverify\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    763\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _resnet(Bottleneck, [\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m6\u001b[39m, \u001b[38;5;241m3\u001b[39m], weights, progress, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\samuelborder\\Desktop\\HIVE_Stuff\\FUSION\\Tools\\fusion-tools\\.venv\\Lib\\site-packages\\torchvision\\models\\_api.py:84\u001b[0m, in \u001b[0;36mWeightsEnum.verify\u001b[1;34m(cls, obj)\u001b[0m\n\u001b[0;32m     82\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m[obj\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, \u001b[38;5;28mcls\u001b[39m):\n\u001b[1;32m---> 84\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m     85\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Weight class provided; expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m but received \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     86\u001b[0m         )\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "\u001b[1;31mTypeError\u001b[0m: Invalid Weight class provided; expected ResNet50_Weights but received EfficientNet_V2_S_Weights."
     ]
    }
   ],
   "source": [
    "\n",
    "model = CellClassificationModel(\n",
    "    output_size = 2,\n",
    "    simple = False\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.Adam([\n",
    "    dict(params = model.parameters(), lr = 5e-5, weight_decay = 0.00001)\n",
    "])\n",
    "\n",
    "loss = torch.nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Starting Training!')\n",
    "print(f'Number of steps per epoch: {round(len(train_data)/batch_size)}')\n",
    "train(\n",
    "    train_data = train_dataloader,\n",
    "    val_data = val_dataloader,\n",
    "    model = model,\n",
    "    optimizer = optimizer,\n",
    "    loss = loss,\n",
    "    output_dir = './outputs/'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Starting Testing!')\n",
    "test(\n",
    "    model_path = './outputs/Classification_Model.pth',\n",
    "    output_size = 2,\n",
    "    holdout_data = val_data,\n",
    "    n = 10\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
